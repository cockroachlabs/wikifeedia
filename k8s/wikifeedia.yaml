# This file contains a deployment and service configuration for a
# generic, HTTP2-based backend. The use of HTTP2 also implies the use of
# TLS between the load-balancer and the backend. This basic
# configuration also works equally well for HTTPS-1.1 or HTTP backends,
# there are callouts below for deletions to make.
# A cronjob to refresh the perf.js file.  It relies on the NFS volume
# to share perf.js and to maintain incremental-sync data.
#
# There are many credentials and secrets which live in a tarball
# contained in the roachperf-sync-homedir k8s Secret. These files
# were copied from the old shared host and get untarred into
# the cockroach user's home directory as part of /entrypoint.sh.
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: wikifeedia-crawl
spec:
  # Disallow concurrent jobs. We impose a maximum runtime below.
  concurrencyPolicy: Forbid
  # Run hourly.
  schedule: "0 * * * *"
  # Must start within 1 minute of expected time, or we'll skip
  # to the next tick.
  startingDeadlineSeconds: 60
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: wikifeedia
        spec:
          containers:
            - name: wikifeedia-crawl
              image: gcr.io/cockroach-dev-inf/cockroachlabs/wikifeedia:master-ajwerner
              imagePullPolicy: Always
              # Guaranteed resources.
              resources:
                requests:
                  cpu: 1
                  memory: 2Gi
                limits:
                  cpu: 1
                  memory: 2Gi
              args: ["wikifeedia", "--pgurl" , "postgres://root@ajwerner-wikifeedia-geo-0001.roachprod.crdb.io:26257?sslmode=disable", "crawl"]
          restartPolicy: Never
      backoffLimit: 1
---

# This deployment object defines a pod template and a replica count.
apiVersion: apps/v1
kind: Deployment
metadata:
  # What's with labels vs names?  The name is how the configuration
  # object is identified by k8s.  The labels are arbitrary key-value
  # pairs, which are used for categorization and selection.  Often,
  # there will be both "app" and "env" keys, or "canary: true" labels.
  labels:
    app: wikifeedia
  name: wikifeedia
spec:
  # This combination of replicas and selector is really saying
  # "ensure that there are this many pods that match the given label selector".
  replicas: 1
  selector:
    matchLabels:
      app: wikifeedia
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: wikifeedia
    spec:
      containers:
        - image: gcr.io/cockroach-dev-inf/cockroachlabs/wikifeedia:master-ajwerner
          imagePullPolicy: Always
          name: wikifeedia
          ports:
            - containerPort: 8080
              protocol: TCP
              # EDITME This name is arbitrary and should be used instead
              # of port numbers when referring to the pod. The general
              # rule is to use the protocol name as the port name.
              name: https
          # Liveness determine if the container should be terminated.
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              # EDITME Ensure this port name and the scheme match what
              # your your app is doing.
              port: https
              scheme: HTTPS
          # Readiness controls when the container is available to serve
          # network requests. For many services, this will be the same
          # query as above. If the backend needs to establish many
          # remote connections or transfer data before actually being
          # able to serve, the use of distinct liveness and readiness
          # probes allows the "failure to launch" case to be detected.
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              # EDITME Ensure this port name and the scheme match what
              # your your app is doing.
              port: https
              scheme: HTTPS
          resources:
            # The requested amount is used to place the pod on a
            # particular machine.
            #
            # EDITME: These values are often determined empirically.
            requests:
              cpu: "500m"
              memory: "1024Mi"
            limits:
              cpu: "500m"
              memory: "1024Mi"
          args: ["wikifeedia", "--pgurl" , "postgres://root@ajwerner-wikifeedia-geo-0001.roachprod.crdb.io:26257?sslmode=disable", "server"]
      terminationGracePeriodSeconds: 30
---
# This service object creates a virtual IP address within the cluster
# that will steer traffic to pods that match a label selector.
apiVersion: v1
kind: Service
metadata:
  annotations:
    # EDITME This line controls which protocol the load balancer will
    # use when connecting to the app. This default app supports HTTP2.
    # Other valid choices are HTTPS or to delete this line for HTTP.
    cloud.google.com/app-protocols: '{"https":"HTTP2"}'
  labels:
    app: wikifeedia
  name: wikifeedia
spec:
  externalTrafficPolicy: Cluster
  ports:
    # EDITME Each Service object generates a virtual IP address, so you'll
    # generally use the default port for the service.
    - name: https
      port: 443
      protocol: TCP
      # EDITME This should be the containerPort.name value.
      targetPort: https
  # This label selector matches against pod labels. Image a case where
  # you have three replicas with a "branch:stable" label and one  with a
  # "branch:canary". Since all four would have an "app:myapp" label, the
  # service will steer traffic between all instances. This requires, of
  # course, that the backend can operate in a mixed-version deployment.
  selector:
    app: wikifeedia
    # This is also a "NodePort" service (as opposed to "LoadBalancer"),
    # which makes every machine in the k8s cluster forward network
    # traffic from an arbitrarily-chosen port number on the host
    # machine's "real" IP address. This is ultimately how the Ingress
    # controller routes HTTP requests into the cluster.
  type: NodePort
